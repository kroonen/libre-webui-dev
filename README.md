# Libre WebUI

A minimalist interface for local LLMs via Ollama.

![Rick Rubin Coding Wisdom](./rr.jpg)

## Setup

```bash
# Option 1: Quick start
./start.sh

# Option 2: Manual
npm install
cd backend && npm install && cd ..
cd frontend && npm install && cd ..
npm run dev
```

## Ports
- Frontend: http://localhost:5173
- Backend: http://localhost:3001
- Ollama: http://localhost:11434

## Features
- Clean, minimal interface
- Light/Dark mode
- Responsive design
- Real-time chat with Ollama models

## License
MIT
